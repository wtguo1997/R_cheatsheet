---
title: "STA206 Fall 2022: Take Home Quiz"
output:
  html_document: default
  word_document: default
---



**Instructions**:  <br>

* In this quiz, you will be asked to perform some  tasks in R <br>
* You should submit a .html (preferred format) or .docx file. 
*  You should only include the output that is directly related to answering the questions. A flood of unprocessed raw output from R may result in penalties.  

In *Quiz_data.Rdata* you will find a data set called *data* with three variables: *Y* and *X1, X2*. For the following, **you should use the original data and no standardization should be applied**. 


* **(a). Load the data into the R workspace. How many observations are there in this data?  **<br>

```{r}
#(Type your code in the space below)
load("Quiz_data.Rdata")
str(data)
which(is.na(data))
```
*(Type your answer here):*
  100 obs. of  3 variables
  
* **(b). What is the type of each variable? For each variable, draw one plot to depict its distribution. Arrange these plots into one multiple paneled graph. **<br> 
```{r}
#(Type your code in the space below)
par(mfrow=c(1,3))
hist(data$Y)
hist(data$X1)
hist(data$X2)
par(mfrow=c(1,1))
sapply(data, class)
```
*(Type your answer here):*
All three variables are numeric.

* **(c). Draw the scatter plot matrix and obtain the correlation matrix for these three variables. Briefly describe how *Y* appears to be related to *X1* and *X2*. **<br>

```{r}
# (Type your code in the space below)
pairs(data)
cor(data)
```

*(Type your answer here):*
Y and X1 are moderately correlated. Y and X2 are moderately correlated.

* **(d). Fit a first-order model with *Y* as the response variable and *X1, X2* as the predictors (referred to as Model 1). How many regression coefficients are there in Model 1? **<br>
```{r}
# (Type your code in the space below)
Model1 <- lm(Y~X1+X2,data=data)
summary(Model1)
```

*(Type your answer here):*
There are 3 regression coefficients in Model1.

* **(e). Conduct model diagnostics for Model 1 and comment on how well model assumptions hold. **<br>
```{r}
# (Type your code in the space below)
par(mfrow=c(2,2))
plot(Model1,which = 1)
plot(Model1,which = 2)
boxplot(Model1$residuals)
par(mfrow=c(1,1))
```

*(Type your answer here):*
The "Residuals vs Fitted" plot shows obvious nonlinearity and Normal Q-Q plot shows that the distribution is right-tailed compare to the normal distribution. Residual boxplot shows range from -3 to 6 and there are a few outliers larger than 3.

* **(f). Fit a 2nd-order polynomial regression model with *Y* as the response variable and *X1, X2* as the predictors (referred to Model 2).   Calculate the variance inflation factors for this model. Does there appears to be strong multicollinearity? Explain briefly.**<br>  


```{r}
# (Type your code in the space below)
Model2 <- lm(Y~X1+X2+X1:X2+I(X1^2)+I(X2^2),data=data)
library(car)
vif(Model2)
```

*(Type your answer here):*
Since $VIF_{X1}>10$, $VIF_{X_2}>10$, $VIF_{X_2^2}>10$, $VIF_{X_1X_2}>10$, there appears to have strong multicollinearity.

* **(g). Conduct model diagnostics for Model 2. Do model assumptions appear to hold better under Model 2 compared to under Model 1?  Explain briefly.  **<br>
```{r}
# (Type your code in the space below)
par(mfrow=c(2,2))
plot(Model2,which = 1)
plot(Model2,which = 2)
boxplot(Model2$residuals)
par(mfrow=c(1,1))
```

*(Type your answer here):*
The "Residuals vs Fitted" plot show no obvious nonlinearity and Normal Q-Q plot shows no severe deviation from Normality. Residual boxplot shows range from -2 to 2. So Model2 fit better than Model1.

* **(h). Under Model 2, obtain the 99% confidence interval for the mean response when $X1=X2=0$. **<br>
```{r}
#(Type your code in the space below)
newX = data.frame(X1=0,X2=0)
predict(Model2, newX, interval="confidence", level=0.99, se.fit=TRUE)
```

*(Type your answer here):*
The 99% confidence interval for the mean response when $X1=X2=0$ is $[3.41719,7.099826]$

* **(i). At the significance level 0.01, test whether or not all terms involving *X2*  may be simultaneously dropped out of Model 2. State your conclusion.  **<br> 

```{r}
#(Type your code in the space below)
Reduced_Model2 <- lm(Y~X1+I(X1^2),data=data)
anova(Model2)
anova(Reduced_Model2)
```

*(Type your answer here):*
$SSE(Model2)=82.541,SSE(Reduced2)=117.899,F^*=\frac{(117.899-82.541)/3}{82.541/94}=13.42223$

$Pvalue=P(F_{3,94}>13.42223)=2.307478\times10^{-7}<0.01$

So all terms involving *X2*  cannot be simultaneously dropped out of Model 2.

* **(j) Find a model that has less regression coefficients AND a larger adjusted coefficient of multiple determination compared to Model 2.  Briefly explain how you reach this model. **<br>

```{r}
#(Type your code in the space below)
Model3 <- lm(Y~X1+X2+I(X1^2),data=data)
summary(Model2)
summary(Model3)
```

*(Type your answer here):*

I reach Model 3 by drop the term with high VIF value to reduce the multicollinearity.