load("Quiz_data.Rdata")
str(data)
which(is.na(data))
# visulization 
par(mfrow=c(1,3))
hist(data$Y)
hist(data$X1)
hist(data$X2)
par(mfrow=c(1,1))
sapply(data, class)

# scatter plot matrix
pairs(data)

# correlation matrix
cor(data)

# First order model
Model1 <- lm(Y~X1+X2,data=data)
summary(Model1)

# model diagnostics
par(mfrow=c(2,2))
plot(Model1,which = 1)
plot(Model1,which = 2)
boxplot(Model1$residuals)
par(mfrow=c(1,1))

# second-order polynomial
Model2 <- lm(Y~X1+X2+X1:X2+I(X1^2)+I(X2^2),data=data)

# variance inflation factors
library(car)
vif(Model2)

# prediction
newX = data.frame(X1=0,X2=0)
predict(Model2, newX, interval="confidence", level=0.99, se.fit=TRUE)

# reduced model
Reduced_Model2 <- lm(Y~X1+I(X1^2),data=data)
anova(Model2)
anova(Reduced_Model2)
# whether we should drop variables dependes on F test: (SSEr - SSEf)/(dfr -dff)/MSEf

# model selection
# see 206 lab 8 and 9

# split dataset
#split data into two halves (training and validation)
set.seed(100)
n <- nrow(d)/2
ind <- sample(1:(2*n), n, replace=FALSE)
train <- d[ind, ] #training set
valid <- d[-ind, ] #validation/test set

# Best subset regression:
library(leaps)# first install this package if you don't have it
sub_set <- regsubsets(y~sqf+bedrm+bathrm+garage, data=train, nbest=6, nvmax=4, method="exhaustive")
sum_sub <- summary(sub_set)
sum_sub
# Calculate AIC/BIC manually
p.m <- rowSums(sum_sub$which) #number of coefficients in each model: p
ssto <- sum((train$y-mean(train$y))^2)
sse <- (1-sum_sub$rsq)*ssto
aic <- n*log(sse/n)+2*p.m
bic <- n*log(sse/n)+log(n)*p.m
res_sub <- cbind(sum_sub$which, sse, sum_sub$rsq, sum_sub$adjr2,sum_sub$cp, bic, aic)

fit0 <- lm(y~1, data=train) #none-model: intercept only
full <- lm(y~sqf+bedrm+bathrm+garage, data=train) #full model
sse0 <- sum(fit0$residuals^2)
p0 <- 1 #only one regression coefficient
c0 <- sse0/summary(full)$sigma^2 - (n-2*p0)
aic0 <- n*log(sse0/n)+2*p0
bic0 <- n*log(sse0/n)+log(n)*p0
none <- c(1,0,0,0,0,sse0,0,0,c0,bic0,aic0) #model summary for intercept model

# combine the results
res_sub <- rbind(none,res_sub)
colnames(res_sub)<-c(colnames(sum_sub$which),"sse", "R^2", "R^2_a", "Cp",
"bic", "aic")
round(res_sub, 2)
p.plot <- res_sub[,1]+res_sub[,2]+res_sub[,3]+res_sub[,4]+res_sub[,5]
res.sub.plot <- as.data.frame(cbind(p.plot, res_sub))
best.plot <- res.sub.plot[c(1,2,6,12,16), ]
par(mfrow = c(3,2))
plot(res.sub.plot$p.plot, res.sub.plot$`R^2`, xlab = "p", ylab = "R^2")
lines(best.plot$p.plot, best.plot$`R^2`, lwd = 2)

plot(res.sub.plot$p.plot, res.sub.plot$`R^2_a`, xlab = "p", ylab = "R^2_a")
lines(best.plot$p.plot, best.plot$`R^2_a`, lwd = 2)

plot(res.sub.plot$p.plot, res.sub.plot$Cp, xlab = "p", ylab = "Cp")
lines(best.plot$p.plot, best.plot$Cp, lwd = 2)
lines(best.plot$p.plot, best.plot$p.plot, col = "red")

plot(res.sub.plot$p.plot, res.sub.plot$aic, xlab = "p", ylab = "aic")
lines(best.plot$p.plot, best.plot$aic, lwd = 2)

# calculate pressP manually
PRESS_none <- sum((fit0$residuals/(1-influence(fit0)$hat))^2)
PRESS_full <- sum((full$residuals/(1-influence(full)$hat))^2)



plot(res.sub.plot$p.plot, res.sub.plot$bic, xlab = "p", ylab = "bic")
lines(best.plot$p.plot, best.plot$bic, lwd = 2)


# stepwise regression
none_mod <- lm(y~1, data=train) ##model with only intercept
full_mod <- lm(y~sqf+bedrm+bathrm+garage+factor(AC)+factor(pool)
+factor(quality)+factor(style)+factor(hw), data=train) ##first order model with 9 predictors 
library(MASS)

# forward selection based on AIC: 
stepAIC(none_mod, scope=list(upper=full_mod, lower = ~1), direction="forward", k=2, trace = FALSE)
# backward elimination based on AIC
stepAIC(full_mod, scope=list(upper=full_mod, lower = ~1), direction="backward", k=2, trace = FALSE)
# forward stepwise based on AIC
stepAIC(none_mod, scope=list(upper=full_mod, lower = ~1), direction="both", k=2, trace = FALSE)
# backward stepwise based on AIC
stepAIC(full_mod, scope=list(upper=full_mod, lower = ~1), direction="both", k=2, trace = FALSE)
# The above models are selected based on AIC. If we want to use BIC, set k=log(n)


# Model validation
train1 <- lm(y ~ sqf + garage + bathrm+ factor(quality), data = train)
valid1 <- lm(y ~ sqf + garage + bathrm+ factor(quality), data = valid)
mod_sum <- cbind(coef(summary(train1))[,1], coef(summary(valid1))[,1],coef(summary(train1))[,2], coef(summary(valid1))[,2])
colnames(mod_sum) <- c("Train Est","Valid Est","Train s.e.","Valid s.e.")
mod_sum
# examine the SSE and adjusted R squares using both the training data and validation data.
sse_t <- sum(train1$residuals^2)
sse_v <- sum(valid1$residuals^2)
Radj_t <- summary(train1)$adj.r.squared
Radj_v <- summary(valid1)$adj.r.squared
train_sum <- c(sse_t,Radj_t)
valid_sum <- c(sse_v,Radj_v)
criteria <- rbind(train_sum,valid_sum)
colnames(criteria) <- c("SSE","R2_adj")
criteria

# get MSPE_V
newdata <- valid[, -1]
y.hat <- predict(train1, newdata)
MSPE <- mean((valid$y - y.hat)^2)
MSPE
# MSPE compare with sse_t/n

# Model diagonstics See lab 9
# Outlying Y
fit <- lm(y ~ sqf + garage, data=train)
e<-fit$residuals ## ordinary residuals 
h<-influence(fit)$hat ##diagonals of the hat matrix: a.k.a. leverage values 
de<-e/(1-h) ##deleted residuals 
plot(e,de, xlab="residuals", ylab="deleted residuals")
abline(0,1)
#  6 largest studentized deleted residuals
library(MASS)
stu.res.del <- studres(fit)
head(sort(abs(stu.res.del), decreasing=TRUE))
# Bonferroni's Threshold at levl 0.1
qt(1-.1/(2*n), n-3-1)

# outlying X (hii > 2p/n)
h <- influence(fit)$hat
p <- 3
sort(h[which(h>2*p/n)], decreasing = TRUE)
# Cook's distance
res <- fit$residuals
mse <- anova(fit)["Residuals", 3]
cook.d <- res^2*h/(p*mse*(1-h)^2)
sort(cook.d[which(cook.d>4/(n-p))], decreasing = TRUE)
# plot cook
plot(fit, which=4)
# plot residual vs leverage
plot(fit, which=5)
